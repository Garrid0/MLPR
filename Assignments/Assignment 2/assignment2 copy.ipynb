{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from ct_support_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.load('data/ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_val:  -0.2160085093241599 with standard error:  0.01290449880016868\n",
      "Mean of first 5,785 entries of y_train:  -0.44247687859693674 with standard error:  0.011927303389170828\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std_err(data):\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(np.size(data)) #used unbiased estimaror for std dev.\n",
    "    return np.mean(data), std_err\n",
    "\n",
    "y_val_mu, y_val_std_err = get_mean_and_std_err(y_val)\n",
    "print(\"Mean of y_val: \", y_val_mu, \"with standard error: \", y_val_std_err)\n",
    "\n",
    "y_train_mu_slice, y_train_std_err_slice = get_mean_and_std_err(y_train[:5785])\n",
    "print(\"Mean of first 5,785 entries of y_train: \", y_train_mu_slice,\n",
    "    \"with standard error: \", y_train_std_err_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_train:  -9.13868774539957e-15 with standard error:  0.0049535309340638205\n"
     ]
    }
   ],
   "source": [
    "#NOT ASKED FOR\n",
    "y_train_mu , y_train_std_err = get_mean_and_std_err(y_train)\n",
    "print(\"Mean of y_train: \",  y_train_mu, \"with standard error: \", y_train_std_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of constant redundant columns:  [59, 69, 179, 189, 351]\n",
      "Indices of subsequent repeated columns after first appearance:  [78, 79, 188, 199, 287, 359]\n"
     ]
    }
   ],
   "source": [
    "all_cols = range(X_train.shape[1])   # Get all col indices.\n",
    "\n",
    "# Get redundant columns with const vals.\n",
    "const_col_ind = []\n",
    "for col_index in all_cols:\n",
    "    if np.std(X_train[:,col_index]) == 0:\n",
    "        const_col_ind.append(col_index)\n",
    "\n",
    "print(\"Indices of constant redundant columns: \", const_col_ind)\n",
    "\n",
    "# Get redundant repeated columns.\n",
    "un_arr, un_ind = np.unique(X_train, return_index=True, axis=1)\n",
    "# Repeated col indices are all except those with constant values and those that are unique.\n",
    "repeated_col_ind = sorted(set(all_cols) - set(const_col_ind) - set(un_ind))\n",
    "\n",
    "print(\"Indices of subsequent repeated columns after first appearance: \", repeated_col_ind)\n",
    "\n",
    "# Remove all that are either repeated or unique.\n",
    "all_cols_to_rmv = list(set(repeated_col_ind) | set(const_col_ind))\n",
    "\n",
    "# Finally process all three arrays to remove redundant features:\n",
    "X_train = np.delete(X_train, all_cols_to_rmv, axis=1)\n",
    "X_val = np.delete(X_val, all_cols_to_rmv, axis=1)\n",
    "X_test = np.delete(X_test, all_cols_to_rmv, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for parameters fitted with lstsq: 0.3567565397204054 and for validation set: 0.4230521968394698\n",
      "RMSE for training set for parameters fitted with gradopt: 0.35675788673184167 and for validation set: 0.42306040065414513\n"
     ]
    }
   ],
   "source": [
    "from ct_support_code import *\n",
    "\n",
    "def fit_linreg(X, yy, k):\n",
    "    \"\"\"Fit linear regression problem using least squares. Return mimicks ct_suppor_codeÂ´s\n",
    "    \n",
    "    I.e return feature weights (Kx1), bias term, and RMSE.\"\"\"\n",
    "    N, K = X.shape\n",
    "\n",
    "    # Modify so lstsq can incorporate regularisation. (Careful not to regularise b).\n",
    "    yy_tilde = np.vstack([yy[:,None], np.zeros((K,1))]) #(N+K) x 1 vector.\n",
    "    X_tilde = np.vstack([X, np.sqrt(alpha)*np.eye(K)])\n",
    "\n",
    "    # Modify design matrix to allow lstsq to also fit bias (constant) term b.\n",
    "    # Will require 1s before original X, and 0s before regularisation diag matrix.\n",
    "    prepend_bias = np.concatenate((np.ones(N),np.zeros(K)))\n",
    "    X_tilde_bias = np.hstack([prepend_bias[:,None], X_tilde])   # (N+K) x (K+1) matrix.\n",
    "\n",
    "    # lstsq(a,b) solves a@x=b. I.e Xw = yy in our case to find optimized weights.\n",
    "    ww_fit, _, _, _ = np.linalg.lstsq(X_tilde_bias, yy_tilde, rcond=None) # K+1 weights for K BFs plus constant term.\n",
    "    return ww_fit[1:], ww_fit[0]\n",
    "\n",
    "ww_lstsq, bb_lstsq = fit_linreg(X_train, y_train, alpha=30)\n",
    "ww_gradopt, bb_gradopt= fit_linreg_gradopt(X_train, y_train, alpha=30)\n",
    "\n",
    "pred_lstsq_train = X_train@ww_lstsq + bb_lstsq     # yy predictions from Xw = yy for our fitted w.\n",
    "pred_lstsq_val = X_val@ww_lstsq + bb_lstsq\n",
    "pred_gradopt_train = X_train@ww_gradopt + bb_gradopt\n",
    "pred_gradopt_val = X_val@ww_gradopt + bb_gradopt\n",
    "\n",
    "RMSE_lstsq_train = np.sqrt(np.mean((y_train[:,None]-pred_lstsq_train)**2))\n",
    "RMSE_gradopt_train = np.sqrt(np.mean((y_train[:,None]-pred_gradopt_train[:,None])**2))\n",
    "RMSE_lstsq_val = np.sqrt(np.mean((y_val[:,None]-pred_lstsq_val)**2))\n",
    "RMSE_gradopt_val = np.sqrt(np.mean((y_val[:,None]-pred_gradopt_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for parameters fitted with lstsq: {RMSE_lstsq_train}\"\n",
    "    f\" and for validation set: {RMSE_lstsq_val}\")\n",
    "print(f\"RMSE for training set for parameters fitted with gradopt: {RMSE_gradopt_train}\"\n",
    "    f\" and for validation set: {RMSE_gradopt_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "  D = X.shape[1]\n",
    "  args = (X, yy, alpha)\n",
    "  init = (np.zeros(D), np.array(0))\n",
    "  ww, bb = minimize_list(logreg_cost, init, args)\n",
    "  return ww, bb\n",
    "\n",
    "def sigma(aa):\n",
    "  \"\"\"Sigmoid function for logreg problems, aa is activation\"\"\"\n",
    "  return 1/(1 + np.exp(-aa))\n",
    "\n",
    "fitted_logreg_ww_vecs = []  # ls of tuples [(ww1,bb1),...,(ww20,bb20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww, bb = fit_logreg_gradopt(X_train, labels, alpha=30)\n",
    "    fitted_logreg_ww_vecs.append((ww, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_logreg_models_train = [sigma(X_train@ww +bb) for ww, bb in fitted_logreg_ww_vecs]\n",
    "# Note even if fitting validation set with logreg models, still use the weights fitted on X_train\n",
    "fitted_logreg_models_val = [sigma(X_val@ww +bb) for ww, bb in fitted_logreg_ww_vecs]\n",
    "\n",
    "X_train_probs = np.asarray(fitted_logreg_models_train).T\n",
    "X_val_probs = np.asarray(fitted_logreg_models_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for parameters fitted first by logreg then linreg: 0.15441150429694728 and for validation set: 0.2542477297869824\n"
     ]
    }
   ],
   "source": [
    "ww_final, bb_final = fit_linreg(X_train_probs, y_train, alpha=30) # use function from earlier\n",
    "\n",
    "# Make predictions on transformed Nx20 X_train\n",
    "pred_final_train = X_train_probs@ww_final + bb_final\n",
    "# Make predictions on transformed Nx20 X_val (using params fitted on X_train)    \n",
    "pred_final_val = X_val_probs@ww_final + bb_final \n",
    "\n",
    "RMSE_final_train = np.sqrt(np.mean((y_train[:,None]-pred_final_train)**2))\n",
    "RMSE_final_val = np.sqrt(np.mean((y_val[:,None]-pred_final_val)**2))\n",
    "\n",
    "print(f\"RMSE for training set for parameters fitted first by logreg then linreg: {RMSE_final_train}\"\n",
    "    f\" and for validation set: {RMSE_final_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for nn params, with random init: 0.13913817503831535 and for validation set: 0.26212876601739454\n",
      "RMSE for training set for nn params, with Q3 init: 0.1396200346812529 and for validation set: 0.26954777171833993\n"
     ]
    }
   ],
   "source": [
    "from ct_support_code import *\n",
    "def fit_nn_gradopt(X, yy, alpha, params=\"rnd\"):   \n",
    "    D = X.shape[1]\n",
    "    K = 20\n",
    "    args = (X, yy, alpha)\n",
    "    if params == \"rnd\":\n",
    "        init = [0.1*np.random.randn(K)/np.sqrt(K), np.array(0), 0.1*np.random.randn(K,D)/np.sqrt(D), np.zeros(K)]\n",
    "    else:\n",
    "        init = params\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "############################## fit neural network with random initialisation #################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=30) # use function from earlier\n",
    "pred_train = nn_cost([ww, bb, V, bk], X_train)\n",
    "pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for nn params, with random init: {RMSE_train}\"\n",
    "    f\" and for validation set: {RMSE_val}\")\n",
    "\n",
    "#################################### Extract parameters fitted in Q3 #########################\n",
    "N, D, K = X_train.shape[0], X_train.shape[1], 20\n",
    "V_Q3 = np.empty((K,D))\n",
    "bk_Q3 = np.empty((K))\n",
    "for k, tuple in enumerate(fitted_logreg_ww_vecs): # list (ww,bb) for each of 20 ks\n",
    "    # for each tuple (corr to one k) break apart the weight vector and bias\n",
    "    ww_vec = tuple[0]   \n",
    "    b_scalar = tuple[1]\n",
    "    # V must have K rows which are the (1xD) weight vector for that basis function/unit\n",
    "    V_Q3[k] = ww_vec\n",
    "    bk_Q3[k] = b_scalar # Nx1 matrix (technically N,)\n",
    "ww_Q3 = ww_final.reshape(K,)\n",
    "bb_Q3 = bb_final\n",
    "\n",
    "params_Q3 = [ww_Q3, bb_Q3, V_Q3, bk_Q3]\n",
    "\n",
    "############################## fit neural network with Q3-optimised initialisation #################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=30, params=params_Q3) \n",
    "\n",
    "pred_train = nn_cost([ww, bb, V, bk], X_train)\n",
    "pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for nn params, with Q3 init: {RMSE_train}\"\n",
    "    f\" and for validation set: {RMSE_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 4.36 to be the regularisation parameter, Î±, that leads to the lowest validation RMSE of 0.23563693814412395.\n",
      "For Î±=4.36 the RMSE on the test set is: 0.28054230889024045.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "################################ Set up Q5 with needed function, baseline, and lists to hold relevant data ####################################\n",
    "def train_nn_reg(alpha):\n",
    "    \"\"\"Fit a Q4 neural network with Q3-optimised parameters and the given alpha for regularisation.\n",
    "    Return the RMSE on the validation set for this fitted nn\"\"\"\n",
    "    ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=alpha, params=params_Q3)\n",
    "    pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "    RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "    return RMSE_val\n",
    "\n",
    "baseline = RMSE_val\n",
    "alphas = np.arange(0, 50, 0.02)\n",
    "obs_alphas = []    # List of chosen alphas we have trained a neural network with\n",
    "obs_alphas_ind = [] # list of the index in original array of alphas already sampled\n",
    "log_RMSE_improvements = []  # List of log(baseline_RMSE) - log(observed_RMSE). Same size as obs_alphas\n",
    "\n",
    "############################ Find 3 alphas randomly sampled from the set of possible alphas to start Bayesian optimization ##############\n",
    "for train_loc in np.random.randint(0, len(alphas), size=3):\n",
    "    alpha = alphas[train_loc]\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha))\n",
    "    obs_alphas.append(alpha)\n",
    "    obs_alphas_ind.append(train_loc)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "\n",
    "# PI function used in Bayesian optimization to find the best regularisation parameter\n",
    "def PI(mu_alpha, sigma_alpha, yys):\n",
    "    best_y = np.max(yys)\n",
    "    return norm.cdf((mu_alpha-best_y)/sigma_alpha)\n",
    "\n",
    "################################ Iterate 5 times to find the best alpha, one with biggest improvement in RMSE ###############################\n",
    "for iteration in range(5):\n",
    "\n",
    "    PI_results = []\n",
    "    # Iterate through the index of unseen alphas_rest to get corresponding mu and sigma.\n",
    "    for index in range(len(alphas_rest)):\n",
    "\n",
    "        mu_alpha = rest_cond_mu[index]  # for each alpha get mean from GP\n",
    "        sigma_alpha = np.sqrt(rest_cond_cov[index][index])   # sqrt(variance) from GP. Variances along diagonal.\n",
    "        # Append the PI for each alpha in the still unseen alphas_rest\n",
    "        PI_results.append(PI(mu_alpha, sigma_alpha, log_RMSE_improvements))\n",
    "\n",
    "    new_alpha = alphas_rest[np.argmax(PI_results)]   # Get the alpha that maximises the PI function\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(new_alpha))\n",
    "    obs_alphas.append(new_alpha)\n",
    "    obs_alphas_ind.append(np.where(alphas == new_alpha)[0][0])    # index of this alpha w.r.t original array of all alphas. (Extract index from np.where)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "    alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "    rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "best_alpha = obs_alphas[np.argmax(log_RMSE_improvements)] # get the alpha with seemingly the biggest improvement in log(RMSE)\n",
    "best_val_RMSE = np.exp(np.log(baseline) - log_RMSE_improvements[np.argmax(log_RMSE_improvements)]) # calc RMSE for validation set for this best alpha\n",
    "print(f'We have found {best_alpha} to be the regularisation parameter, Î±, that leads to the lowest validation RMSE of {best_val_RMSE}.')\n",
    "\n",
    "################################ Calculate the RMSE for the Test set using this alpha #######################################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=best_alpha, params=params_Q3)\n",
    "pred_test = nn_cost([ww, bb, V, bk], X_test)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "print(f'For Î±={best_alpha} the RMSE on the test set is: {RMSE_test}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Possible Improvements**\n",
    "* Thus, it is recommended to scale your data to values between 0 and 1 (e.g. by using MinMaxScaler from Scikit-Learn).\n",
    "* Transforming before multilevel modeling (thus attempting to make coefficients more comparable, thus allowing more effective second-level regressions, which in turn improve partial pooling).\n",
    "* Tune other hyperparameters, namely decreasing K=20 by doing bayesian optimization between 0 and 30.\n",
    "* Change unit type to relu to avoid vanishing gradients\n",
    "    * Instead get dead ReLUs but may be good as it drops nodes from over-complex neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q6** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find model generalises badly. Thus need to fit a less flexible model (e.g decrease size of hidden layer). Also change from PI to EI function to have explicit tradeoff between exploration and exploitation, i.e have a measure of how much we improve by not only how lilely we are to improve. Then to Bayesian optimization of hyperparameters alpha (regularisation) and K (size of hidden layer).  Finally we swap unit type to ReLU from sigmoid for two reasons (general) in which sigmoid has higher likelihood of vanishing gradient (i.e no substantial weight changes for high activations) and more specifically: sparsity (which is good for overly flexible nn with bad generalisation as some units do not take part in learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 17 to be the the best layer size parameter, K, that leads to the lowest validation RMSE of 0.196760206204376.\n",
      "For K=17 the RMSE on the test set is: 0.2660461821551051. (Note: RMSE_train=0.07238112504423648)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from ct_support_code import *\n",
    "\n",
    "############################ Fitting function that letÂ´s us set K and alpha hyperparameters ###########################\n",
    "def fit_nn_gradopt(X, yy, alpha=30, k_size=20, params=\"rnd\"):   \n",
    "    D = X.shape[1]\n",
    "    K = k_size  # No. units in hidden layers\n",
    "    args = (X, yy, alpha)\n",
    "    # Need rnd init if using ReLUs (using sigmoid in Q3) and/or if changing size of hidden layer, previously (in Q3) K=20.\n",
    "    if params == \"rnd\":\n",
    "        init = [0.1*np.random.randn(K)/np.sqrt(K), np.array(0), 0.1*np.random.randn(K,D)/np.sqrt(D), np.zeros(K)]\n",
    "    else:\n",
    "        init = params\n",
    "    ww, bb, V, bk = minimize_list(nn_cost_relu, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "########################### Set up overseeing function, baseline, and lists to hold relevant data #########################\n",
    "def train_nn_reg(alpha=30, k_size=20, fitting=\"alpha\"):\n",
    "    \"\"\"Fit a random init neural network with regularisation alpha and a hidden layer of size K.\n",
    "    Return the RMSE on the validation set for this fitted nn\"\"\"\n",
    "    ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha, k_size, params=\"rnd\") # need rnd init as donÂ´t know K\n",
    "    # DonÂ´t want to fit K and then alpha on same validation data as sure to overfit.\n",
    "    # Split validation but keep order to avoid shuffling.\n",
    "    if fitting==\"K\":\n",
    "        pred_val = nn_cost_relu([ww, bb, V, bk], np.array_split(X_val,2)[0])\n",
    "        RMSE_val = np.sqrt(np.mean((np.array_split(y_val,2)[0][:,None]-pred_val[:,None])**2))\n",
    "    else:\n",
    "        pred_val = nn_cost_relu([ww, bb, V, bk], np.array_split(X_val,2)[1])\n",
    "        RMSE_val = np.sqrt(np.mean((np.array_split(y_val,2)[1][:,None]-pred_val[:,None])**2))\n",
    "    return RMSE_val\n",
    "\n",
    "# EI function used in Bayesian optimization to avoid costly gridsearch over parameter space.\n",
    "# Better than PI as also selects for values with a bigger improvement.\n",
    "def EI(mu, sigma, yys):\n",
    "    best_y = np.max(yys)\n",
    "    Z = (mu-best_y)/sigma\n",
    "    return (mu - best_y)*norm.cdf(Z) + sigma*norm.pdf(Z)\n",
    "\n",
    "##########################################################################################################################\n",
    "############################################## FIT Ks FIRST. As dictates network layout. #################################\n",
    "##########################################################################################################################\n",
    "\n",
    "baseline =  0.26954777171833993\n",
    "ks = np.arange(0, 31, 1)    # sample Ks around K=20 (expect probably smaller)\n",
    "obs_ks = []    # List of chosen ks we have trained a neural network with\n",
    "obs_ks_ind = [] # list of the index in original array of ks already sampled\n",
    "log_RMSE_improvements = []  # List of log(baseline_RMSE) - log(observed_RMSE). Same size as obs_ks\n",
    "\n",
    "##################### Find 3 ks randomly sampled from the set of possible ks to start Bayesian optimization ##############\n",
    "for train_loc in np.random.randint(0, len(ks), size=3):\n",
    "    k = ks[train_loc]\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha=30, k_size=k, fitting=\"K\"))\n",
    "    obs_ks.append(k)\n",
    "    obs_ks_ind.append(train_loc)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "ks_rest = np.delete(ks, obs_ks_ind)\n",
    "rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=ks_rest, X_obs=np.array(obs_ks), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "################################ Iterate 5 times to find the best K: biggest improvement in validation RMSE ######################\n",
    "for iteration in range(5):\n",
    "\n",
    "    EI_results = []\n",
    "    # Iterate through the index of unseen ks_rest to get corresponding mu and sigma.\n",
    "    for index in range(len(ks_rest)):\n",
    "\n",
    "        mu_k = rest_cond_mu[index]  # for each k get mean from GP\n",
    "        sigma_k = np.sqrt(rest_cond_cov[index][index])   # sqrt(variance) from GP. Variances along diagonal.\n",
    "        # Append the EI for each K in the still unseen k_rest\n",
    "        EI_results.append(EI(mu_k, sigma_k, log_RMSE_improvements))\n",
    "\n",
    "    new_k = ks_rest[np.argmax(EI_results)]   # Get the k that maximises the EI function\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha=30, k_size=new_k, fitting=\"K\"))\n",
    "    obs_ks.append(new_k)\n",
    "    obs_ks_ind.append(np.where(ks == new_k)[0][0])    # index of this k w.r.t original array of all ks. (Extract index from np.where)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "    ks_rest = np.delete(ks, obs_ks_ind)\n",
    "    rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=ks_rest, X_obs=np.array(obs_ks), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "best_k = obs_ks[np.argmax(log_RMSE_improvements)] # get the k with seemingly the biggest improvement in log(RMSE)\n",
    "best_val_RMSE = np.exp(np.log(baseline) - log_RMSE_improvements[np.argmax(log_RMSE_improvements)]) # calc RMSE for validation set for this best k\n",
    "print(f'We have found {best_k} to be the the best layer size parameter, K, that leads to the lowest validation RMSE of {best_val_RMSE}.')\n",
    "\n",
    "################################ Calculate the RMSE for the Test set using this k #######################################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, k_size=best_k, params=\"rnd\")\n",
    "pred_test = nn_cost_relu([ww, bb, V, bk], X_test)\n",
    "pred_train = nn_cost_relu([ww, bb, V, bk], X_train)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "print(f'For K={best_k} the RMSE on the test set is: {RMSE_test}. (Note: RMSE_train={RMSE_train})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 38.26 to be the regularisation parameter, Î±, (for K=17) that leads to the lowest validation RMSE of 0.2665172246186848.\n",
      "For K=17 the RMSE on the test set is: 0.2674947829894205. (Note: RMSE_train=0.07694144350247213)\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "############################################## FIT alphas SECOND. As now regularise chosen network. ######################\n",
    "##########################################################################################################################\n",
    "\n",
    "baseline =  0.26954777171833993\n",
    "alphas = np.arange(0, 50, 0.02)\n",
    "obs_alphas = []    # List of chosen alphas we have trained a neural network with\n",
    "obs_alphas_ind = [] # list of the index in original array of alphas already sampled\n",
    "log_RMSE_improvements = []  # List of log(baseline_RMSE) - log(observed_RMSE). Same size as obs_alphas\n",
    "\n",
    "############################ Find 3 alphas randomly sampled from the set of possible alphas to start Bayesian optimization ##############\n",
    "for train_loc in np.random.randint(0, len(alphas), size=3):\n",
    "    alpha = alphas[train_loc]\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha=alpha, k_size=best_k, fitting=\"alpha\"))\n",
    "    obs_alphas.append(alpha)\n",
    "    obs_alphas_ind.append(train_loc)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "################################ Iterate 5 times to find the best K: biggest improvement in validation RMSE ######################\n",
    "for iteration in range(5):\n",
    "\n",
    "    EI_results = []\n",
    "    # Iterate through the index of unseen alphas_rest to get corresponding mu and sigma.\n",
    "    for index in range(len(alphas_rest)):\n",
    "        mu_alpha = rest_cond_mu[index]  # for each alpha get mean from GP\n",
    "        sigma_alpha = np.sqrt(rest_cond_cov[index][index])   # sqrt(variance) from GP. Variances along diagonal.\n",
    "        # Append the EI for each alpha in the still unseen alphas_rest\n",
    "        EI_results.append(EI(mu_alpha, sigma_alpha, log_RMSE_improvements))\n",
    "\n",
    "    new_alpha = alphas_rest[np.argmax(EI_results)]   # Get the alpha that maximises the EI function\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha=new_alpha, k_size=best_k, fitting=\"alpha\"))\n",
    "    obs_alphas.append(new_alpha)\n",
    "    obs_alphas_ind.append(np.where(alphas == new_alpha)[0][0])    # index of this alpha w.r.t original array of all alphas. (Extract index from np.where)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "    alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "    rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "best_alpha = obs_alphas[np.argmax(log_RMSE_improvements)] # get the alpha with seemingly the biggest improvement in log(RMSE)\n",
    "best_val_RMSE = np.exp(np.log(baseline) - log_RMSE_improvements[np.argmax(log_RMSE_improvements)]) # calc RMSE for validation set for this best alpha\n",
    "print(f'We have found {best_alpha} to be the regularisation parameter, Î±, (for K={best_k}) ' +\n",
    "      f'that leads to the lowest validation RMSE of {best_val_RMSE}.')\n",
    "\n",
    "################################ Calculate the RMSE for the test set using this alpha and k combo #########################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=best_alpha, k_size=best_k, params=\"rnd\")\n",
    "pred_test = nn_cost_relu([ww, bb, V, bk], X_test)\n",
    "pred_train = nn_cost_relu([ww, bb, V, bk], X_train)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "print(f'For K={best_k} the RMSE on the test set is: {RMSE_test}. (Note: RMSE_train={RMSE_train})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1311890f10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(alphas_rest, rest_cond_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.018550664939062544,\n",
       " 0.02692823823428281,\n",
       " 0.054546916027363945,\n",
       " -0.06275970582060642,\n",
       " 0.0598952799254322,\n",
       " -0.03870761302542225,\n",
       " 0.027711550899556237,\n",
       " -0.04945887848461883]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_RMSE_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K=19 the RMSE on the test set is: 0.25250715047972916. (Note: RMSE_train=0.05927932079017135)\n"
     ]
    }
   ],
   "source": [
    "################################ Calculate the RMSE for the test set using this alpha and k combo #########################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=10.82, k_size=best_k, params=\"rnd\")\n",
    "pred_test = nn_cost_relu([ww, bb, V, bk], X_test)\n",
    "pred_train = nn_cost_relu([ww, bb, V, bk], X_train)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "print(f'For K={best_k} the RMSE on the test set is: {RMSE_test}. (Note: RMSE_train={RMSE_train})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yess\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19302056105248253"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_nn_reg(alpha=30, k_size=20, fitting=\"alpha\"):\n",
    "    \"\"\"Fit a random init neural network with regularisation alpha and a hidden layer of size K.\n",
    "    Return the RMSE on the validation set for this fitted nn\"\"\"\n",
    "    #ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha, k_size, params=\"rnd\") # need rnd init as donÂ´t know K\n",
    "    # DonÂ´t want to fit K and then alpha on same validation data as sure to overfit.\n",
    "    # Split validation but keep order to avoid shuffling.\n",
    "    if fitting==\"K\":\n",
    "        print(\"yess\")\n",
    "        pred_val = nn_cost_relu([ww, bb, V, bk], np.array_split(X_val,2)[0])\n",
    "        RMSE_val = np.sqrt(np.mean((np.array_split(y_val,2)[0][:,None]-pred_val[:,None])**2))\n",
    "    else:\n",
    "        print(\"Noo\")\n",
    "        pred_val = nn_cost_relu([ww, bb, V, bk], np.array_split(X_val,2)[1])\n",
    "        RMSE_val = np.sqrt(np.mean((np.array_split(y_val,2)[1][:,None]-pred_val[:,None])**2))\n",
    "    return RMSE_val\n",
    "\n",
    "train_nn_reg(alpha=new_alpha, k_size=best_k, fitting=\"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2893,373) (2892,373) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3625/1910177891.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2893,373) (2892,373) "
     ]
    }
   ],
   "source": [
    "np.array_split(X_val,2)[0]-np.array_split(X_val,2)[1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
