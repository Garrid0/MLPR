{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "from ct_support_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.load('data/ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_val:  -0.2160085093241599 with standard error:  0.01290449880016868\n",
      "Mean of first 5,785 entries of y_train:  -0.44247687859693674 with standard error:  0.011927303389170828\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std_err(data):\n",
    "    std_err = np.std(data, ddof=1) / np.sqrt(np.size(data)) #used unbiased estimaror for std dev.\n",
    "    return np.mean(data), std_err\n",
    "\n",
    "y_val_mu, y_val_std_err = get_mean_and_std_err(y_val)\n",
    "print(\"Mean of y_val: \", y_val_mu, \"with standard error: \", y_val_std_err)\n",
    "\n",
    "y_train_mu_slice, y_train_std_err_slice = get_mean_and_std_err(y_train[:5785])\n",
    "print(\"Mean of first 5,785 entries of y_train: \", y_train_mu_slice,\n",
    "    \"with standard error: \", y_train_std_err_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of y_train:  -9.13868774539957e-15 with standard error:  0.0049535309340638205\n"
     ]
    }
   ],
   "source": [
    "#NOT ASKED FOR\n",
    "y_train_mu , y_train_std_err = get_mean_and_std_err(y_train)\n",
    "print(\"Mean of y_train: \",  y_train_mu, \"with standard error: \", y_train_std_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of constant redundant columns:  [59, 69, 179, 189, 351]\n",
      "Indices of subsequent repeated columns after first appearance:  [78, 79, 188, 199, 287, 359]\n"
     ]
    }
   ],
   "source": [
    "all_cols = range(X_train.shape[1])   # Get all col indices.\n",
    "\n",
    "# Get redundant columns with const vals.\n",
    "const_col_ind = []\n",
    "for col_index in all_cols:\n",
    "    if np.std(X_train[:,col_index]) == 0:\n",
    "        const_col_ind.append(col_index)\n",
    "\n",
    "print(\"Indices of constant redundant columns: \", const_col_ind)\n",
    "\n",
    "# Get redundant repeated columns.\n",
    "un_arr, un_ind = np.unique(X_train, return_index=True, axis=1)\n",
    "# Repeated col indices are all except those with constant values and those that are unique.\n",
    "repeated_col_ind = sorted(set(all_cols) - set(const_col_ind) - set(un_ind))\n",
    "\n",
    "print(\"Indices of subsequent repeated columns after first appearance: \", repeated_col_ind)\n",
    "\n",
    "# Remove all that are either repeated or unique.\n",
    "all_cols_to_rmv = list(set(repeated_col_ind) | set(const_col_ind))\n",
    "\n",
    "# Finally process all three arrays to remove redundant features:\n",
    "X_train = np.delete(X_train, all_cols_to_rmv, axis=1)\n",
    "X_val = np.delete(X_val, all_cols_to_rmv, axis=1)\n",
    "X_test = np.delete(X_test, all_cols_to_rmv, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for parameters fitted with lstsq: 0.3567565397204054 and for validation set: 0.4230521968394698\n",
      "RMSE for training set for parameters fitted with gradopt: 0.35675788673184167 and for validation set: 0.42306040065414513\n"
     ]
    }
   ],
   "source": [
    "from ct_support_code import *\n",
    "\n",
    "def fit_linreg(X, yy, k):\n",
    "    \"\"\"Fit linear regression problem using least squares. Return mimicks ct_suppor_codeÂ´s\n",
    "    \n",
    "    I.e return feature weights (Kx1), bias term, and RMSE.\"\"\"\n",
    "    N, K = X.shape\n",
    "\n",
    "    # Modify so lstsq can incorporate regularisation. (Careful not to regularise b).\n",
    "    yy_tilde = np.vstack([yy[:,None], np.zeros((K,1))]) #(N+K) x 1 vector.\n",
    "    X_tilde = np.vstack([X, np.sqrt(alpha)*np.eye(K)])\n",
    "\n",
    "    # Modify design matrix to allow lstsq to also fit bias (constant) term b.\n",
    "    # Will require 1s before original X, and 0s before regularisation diag matrix.\n",
    "    prepend_bias = np.concatenate((np.ones(N),np.zeros(K)))\n",
    "    X_tilde_bias = np.hstack([prepend_bias[:,None], X_tilde])   # (N+K) x (K+1) matrix.\n",
    "\n",
    "    # lstsq(a,b) solves a@x=b. I.e Xw = yy in our case to find optimized weights.\n",
    "    ww_fit, _, _, _ = np.linalg.lstsq(X_tilde_bias, yy_tilde, rcond=None) # K+1 weights for K BFs plus constant term.\n",
    "    return ww_fit[1:], ww_fit[0]\n",
    "\n",
    "ww_lstsq, bb_lstsq = fit_linreg(X_train, y_train, alpha=30)\n",
    "ww_gradopt, bb_gradopt= fit_linreg_gradopt(X_train, y_train, alpha=30)\n",
    "\n",
    "pred_lstsq_train = X_train@ww_lstsq + bb_lstsq     # yy predictions from Xw = yy for our fitted w.\n",
    "pred_lstsq_val = X_val@ww_lstsq + bb_lstsq\n",
    "pred_gradopt_train = X_train@ww_gradopt + bb_gradopt\n",
    "pred_gradopt_val = X_val@ww_gradopt + bb_gradopt\n",
    "\n",
    "RMSE_lstsq_train = np.sqrt(np.mean((y_train[:,None]-pred_lstsq_train)**2))\n",
    "RMSE_gradopt_train = np.sqrt(np.mean((y_train[:,None]-pred_gradopt_train[:,None])**2))\n",
    "RMSE_lstsq_val = np.sqrt(np.mean((y_val[:,None]-pred_lstsq_val)**2))\n",
    "RMSE_gradopt_val = np.sqrt(np.mean((y_val[:,None]-pred_gradopt_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for parameters fitted with lstsq: {RMSE_lstsq_train}\"\n",
    "    f\" and for validation set: {RMSE_lstsq_val}\")\n",
    "print(f\"RMSE for training set for parameters fitted with gradopt: {RMSE_gradopt_train}\"\n",
    "    f\" and for validation set: {RMSE_gradopt_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "  D = X.shape[1]\n",
    "  args = (X, yy, alpha)\n",
    "  init = (np.zeros(D), np.array(0))\n",
    "  ww, bb = minimize_list(logreg_cost, init, args)\n",
    "  return ww, bb\n",
    "\n",
    "def sigma(aa):\n",
    "  \"\"\"Sigmoid function for logreg problems, aa is activation\"\"\"\n",
    "  return 1/(1 + np.exp(-aa))\n",
    "\n",
    "fitted_logreg_ww_vecs = []  # ls of tuples [(ww1,bb1),...,(ww20,bb20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    ww, bb = fit_logreg_gradopt(X_train, labels, alpha=30)\n",
    "    fitted_logreg_ww_vecs.append((ww, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_logreg_models_train = [sigma(X_train@ww +bb) for ww, bb in fitted_logreg_ww_vecs]\n",
    "# Note even if fitting validation set with logreg models, still use the weights fitted on X_train\n",
    "fitted_logreg_models_val = [sigma(X_val@ww +bb) for ww, bb in fitted_logreg_ww_vecs]\n",
    "\n",
    "X_train_probs = np.asarray(fitted_logreg_models_train).T\n",
    "X_val_probs = np.asarray(fitted_logreg_models_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for parameters fitted first by logreg then linreg: 0.15441150429694728 and for validation set: 0.2542477297869824\n"
     ]
    }
   ],
   "source": [
    "ww_final, bb_final = fit_linreg(X_train_probs, y_train, alpha=30) # use function from earlier\n",
    "\n",
    "# Make predictions on transformed Nx20 X_train\n",
    "pred_final_train = X_train_probs@ww_final + bb_final\n",
    "# Make predictions on transformed Nx20 X_val (using params fitted on X_train)    \n",
    "pred_final_val = X_val_probs@ww_final + bb_final \n",
    "\n",
    "RMSE_final_train = np.sqrt(np.mean((y_train[:,None]-pred_final_train)**2))\n",
    "RMSE_final_val = np.sqrt(np.mean((y_val[:,None]-pred_final_val)**2))\n",
    "\n",
    "print(f\"RMSE for training set for parameters fitted first by logreg then linreg: {RMSE_final_train}\"\n",
    "    f\" and for validation set: {RMSE_final_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training set for nn params, with random init: 0.13913817503831535 and for validation set: 0.26212876601739454\n",
      "RMSE for training set for nn params, with Q3 init: 0.1396200346812529 and for validation set: 0.26954777171833993\n"
     ]
    }
   ],
   "source": [
    "from ct_support_code import *\n",
    "def fit_nn_gradopt(X, yy, alpha, params=\"rnd\"):   \n",
    "    D = X.shape[1]\n",
    "    K = 20\n",
    "    args = (X, yy, alpha)\n",
    "    if params == \"rnd\":\n",
    "        init = [0.1*np.random.randn(K)/np.sqrt(K), np.array(0), 0.1*np.random.randn(K,D)/np.sqrt(D), np.zeros(K)]\n",
    "    else:\n",
    "        init = params\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "############################## fit neural network with random initialisation #################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=30) # use function from earlier\n",
    "pred_train = nn_cost([ww, bb, V, bk], X_train)\n",
    "pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for nn params, with random init: {RMSE_train}\"\n",
    "    f\" and for validation set: {RMSE_val}\")\n",
    "\n",
    "#################################### Extract parameters fitted in Q3 #########################\n",
    "N, D, K = X_train.shape[0], X_train.shape[1], 20\n",
    "V_Q3 = np.empty((K,D))\n",
    "bk_Q3 = np.empty((K))\n",
    "for k, tuple in enumerate(fitted_logreg_ww_vecs): # list (ww,bb) for each of 20 ks\n",
    "    # for each tuple (corr to one k) break apart the weight vector and bias\n",
    "    ww_vec = tuple[0]   \n",
    "    b_scalar = tuple[1]\n",
    "    # V must have K rows which are the (1xD) weight vector for that basis function/unit\n",
    "    V_Q3[k] = ww_vec\n",
    "    bk_Q3[k] = b_scalar # Nx1 matrix (technically N,)\n",
    "ww_Q3 = ww_final.reshape(K,)\n",
    "bb_Q3 = bb_final\n",
    "\n",
    "params_Q3 = [ww_Q3, bb_Q3, V_Q3, bk_Q3]\n",
    "\n",
    "############################## fit neural network with Q3-optimised initialisation #################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=30, params=params_Q3) \n",
    "\n",
    "pred_train = nn_cost([ww, bb, V, bk], X_train)\n",
    "pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "\n",
    "RMSE_train = np.sqrt(np.mean((y_train[:,None]-pred_train[:,None])**2))\n",
    "RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "\n",
    "print(f\"RMSE for training set for nn params, with Q3 init: {RMSE_train}\"\n",
    "    f\" and for validation set: {RMSE_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 4.36 to be the regularisation parameter, Î±, that leads to the lowest validation RMSE of 0.23563693814412395.\n",
      "For Î±=4.36 the RMSE on the test set is: 0.28054230889024045.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "################################ Set up Q5 with needed function, baseline, and lists to hold relevant data ####################################\n",
    "def train_nn_reg(alpha):\n",
    "    \"\"\"Fit a Q4 neural network with Q3-optimised parameters and the given alpha for regularisation.\n",
    "    Return the RMSE on the validation set for this fitted nn\"\"\"\n",
    "    ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=alpha, params=params_Q3)\n",
    "    pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "    RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "    return RMSE_val\n",
    "\n",
    "baseline = RMSE_val\n",
    "alphas = np.arange(0, 50, 0.02)\n",
    "obs_alphas = []    # List of chosen alphas we have trained a neural network with\n",
    "obs_alphas_ind = [] # list of the index in original array of alphas already sampled\n",
    "log_RMSE_improvements = []  # List of log(baseline_RMSE) - log(observed_RMSE). Same size as obs_alphas\n",
    "\n",
    "############################ Find 3 alphas randomly sampled from the set of possible alphas to start Bayesian optimization ##############\n",
    "for train_loc in np.random.randint(0, len(alphas), size=3):\n",
    "    alpha = alphas[train_loc]\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(alpha))\n",
    "    obs_alphas.append(alpha)\n",
    "    obs_alphas_ind.append(train_loc)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "\n",
    "# PI function used in Bayesian optimization to find the best regularisation parameter\n",
    "def PI(mu_alpha, sigma_alpha, yys):\n",
    "    best_y = np.max(yys)\n",
    "    return norm.cdf((mu_alpha-best_y)/sigma_alpha)\n",
    "\n",
    "################################ Iterate 5 times to find the best alpha, one with biggest improvement in RMSE ###############################\n",
    "for iteration in range(5):\n",
    "\n",
    "    PI_results = []\n",
    "    # Iterate through the index of unseen alphas_rest to get corresponding mu and sigma.\n",
    "    for index in range(len(alphas_rest)):\n",
    "\n",
    "        mu_alpha = rest_cond_mu[index]  # for each alpha get mean from GP\n",
    "        sigma_alpha = np.sqrt(rest_cond_cov[index][index])   # sqrt(variance) from GP. Variances along diagonal.\n",
    "        # Append the PI for each alpha in the still unseen alphas_rest\n",
    "        PI_results.append(PI(mu_alpha, sigma_alpha, log_RMSE_improvements))\n",
    "\n",
    "    new_alpha = alphas_rest[np.argmax(PI_results)]   # Get the alpha that maximises the PI function\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(new_alpha))\n",
    "    obs_alphas.append(new_alpha)\n",
    "    obs_alphas_ind.append(np.where(alphas == new_alpha)[0][0])    # index of this alpha w.r.t original array of all alphas. (Extract index from np.where)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "    alphas_rest = np.delete(alphas, obs_alphas_ind)\n",
    "    rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=alphas_rest, X_obs=np.array(obs_alphas), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "best_alpha = obs_alphas[np.argmax(log_RMSE_improvements)] # get the alpha with seemingly the biggest improvement in log(RMSE)\n",
    "best_val_RMSE = np.exp(np.log(baseline) - log_RMSE_improvements[np.argmax(log_RMSE_improvements)]) # calc RMSE for validation set for this best alpha\n",
    "print(f'We have found {best_alpha} to be the regularisation parameter, Î±, that leads to the lowest validation RMSE of {best_val_RMSE}.')\n",
    "\n",
    "################################ Calculate the RMSE for the Test set using this alpha #######################################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha=best_alpha, params=params_Q3)\n",
    "pred_test = nn_cost([ww, bb, V, bk], X_test)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "print(f'For Î±={best_alpha} the RMSE on the test set is: {RMSE_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alphas_rest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1602/891687916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# find out what the gaussian process believes about the alpha with the largest improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas_rest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest_cond_mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11.46\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alphas_rest' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# find out what the gaussian process believes about the alpha with the largest improvement\n",
    "plt.plot(alphas_rest, rest_cond_mu)\n",
    "plt.axvline(x=11.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.06, 16.7, 15.26, 18.06, 12.84, 11.44, 11.42, 11.46]\n",
      "[-0.005730374071766686, 0.05078695769700481, 0.042111008991911, 0.017764585445980474, 0.06610610161287567, 0.0565280179450911, 0.059111894113883956, 0.06747829736822464]\n"
     ]
    }
   ],
   "source": [
    "print(obs_alphas)\n",
    "print(log_RMSE_improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Possible Improvements**\n",
    "* Thus, it is recommended to scale your data to values between 0 and 1 (e.g. by using MinMaxScaler from Scikit-Learn).\n",
    "* Transforming before multilevel modeling (thus attempting to make coefficients more comparable, thus allowing more effective second-level regressions, which in turn improve partial pooling).\n",
    "* Tune other hyperparameters, namely decreasing K=20 by doing bayesian optimization between 0 and 30.\n",
    "* Change unit type to relu to avoid vanishing gradients\n",
    "    * Instead get dead ReLUs but may be good as it drops nodes from over-complex neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q6** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 2 to be the the best layer size parameter, K, that leads to the lowest validation RMSE of 0.24862308508151626.\n",
      "For K=2 the RMSE on the test set is: 0.37992933121157596.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "################################ Get new fitting function that also letÂ´s us set K #############################\n",
    "def fit_nn_gradopt(X, yy, alpha=30, k_size=20, params=\"rnd\"):   \n",
    "    D = X.shape[1]\n",
    "    K = k_size  # No. units in hidden layers\n",
    "    args = (X, yy, alpha)\n",
    "    if params == \"rnd\":\n",
    "        init = [0.1*np.random.randn(K)/np.sqrt(K), np.array(0), 0.1*np.random.randn(K,D)/np.sqrt(D), np.zeros(K)]\n",
    "    else:\n",
    "        init = params\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "################################ Set up overseeing function, baseline, and lists to hold relevant data ####################################\n",
    "def train_nn_reg(alpha=30, k_size=20):\n",
    "    \"\"\"Fit a random init neural network with regularisation and a hidden layer of size K.\n",
    "    Return the RMSE on the validation set for this fitted nn\"\"\"\n",
    "    ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, alpha, k_size, params=\"rnd\") # need rnd init as donÂ´t know K\n",
    "    pred_val = nn_cost([ww, bb, V, bk], X_val)\n",
    "    RMSE_val = np.sqrt(np.mean((y_val[:,None]-pred_val[:,None])**2))\n",
    "    return RMSE_val\n",
    "\n",
    "baseline = RMSE_val\n",
    "ks = np.arange(0, 31, 1)\n",
    "obs_ks = []    # List of chosen ks we have trained a neural network with\n",
    "obs_ks_ind = [] # list of the index in original array of ks already sampled\n",
    "log_RMSE_improvements = []  # List of log(baseline_RMSE) - log(observed_RMSE). Same size as obs_ks\n",
    "\n",
    "############################ Find 3 ks randomly sampled from the set of possible ks to start Bayesian optimization ##############\n",
    "for train_loc in np.random.randint(0, len(ks), size=3):\n",
    "    k = ks[train_loc]\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(k_size=k))\n",
    "    obs_ks.append(k)\n",
    "    obs_ks_ind.append(train_loc)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "ks_rest = np.delete(ks, obs_ks_ind)\n",
    "rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=ks_rest, X_obs=np.array(obs_ks), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "# EI function used in Bayesian optimization to find the best regularisation parameter.\n",
    "def EI(mu_k, sigma_k, yys):\n",
    "    best_y = np.max(yys)\n",
    "    Z = (mu_k-best_y)/sigma_k\n",
    "    return (mu_k - best_y)*norm.cdf(Z) + sigma_k*norm.pdf(Z)\n",
    "\n",
    "################################ Iterate 5 times to find the best K: biggest improvement in validation RMSE ######################\n",
    "for iteration in range(5):\n",
    "\n",
    "    PI_results = []\n",
    "    # Iterate through the index of unseen ks_rest to get corresponding mu and sigma.\n",
    "    for index in range(len(ks_rest)):\n",
    "\n",
    "        mu_k = rest_cond_mu[index]  # for each k get mean from GP\n",
    "        sigma_k = np.sqrt(rest_cond_cov[index][index])   # sqrt(variance) from GP. Variances along diagonal.\n",
    "        # Append the PI for each K in the still unseen k_rest\n",
    "        PI_results.append(PI(mu_k, sigma_k, log_RMSE_improvements))\n",
    "\n",
    "    new_k = ks_rest[np.argmax(PI_results)]   # Get the k that maximises the EI function\n",
    "    yy = np.log(baseline) - np.log(train_nn_reg(new_k))\n",
    "    obs_ks.append(new_k)\n",
    "    obs_ks_ind.append(np.where(ks == new_k)[0][0])    # index of this k w.r.t original array of all ks. (Extract index from np.where)\n",
    "    log_RMSE_improvements.append(yy)\n",
    "\n",
    "    ks_rest = np.delete(ks, obs_ks_ind)\n",
    "    rest_cond_mu, rest_cond_cov = gp_post_par(X_rest=ks_rest, X_obs=np.array(obs_ks), yy=np.array(log_RMSE_improvements))\n",
    "\n",
    "best_k = obs_ks[np.argmax(log_RMSE_improvements)] # get the k with seemingly the biggest improvement in log(RMSE)\n",
    "best_val_RMSE = np.exp(np.log(baseline) - log_RMSE_improvements[np.argmax(log_RMSE_improvements)]) # calc RMSE for validation set for this best k\n",
    "print(f'We have found {best_k} to be the the best layer size parameter, K, that leads to the lowest validation RMSE of {best_val_RMSE}.')\n",
    "\n",
    "################################ Calculate the RMSE for the Test set using this k #######################################################\n",
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, k_size=best_k, params=\"rnd\")\n",
    "pred_test = nn_cost([ww, bb, V, bk], X_test)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "print(f'For K={best_k} the RMSE on the test set is: {RMSE_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K=19 the RMSE on the test set is: 0.2911682180153243.\n"
     ]
    }
   ],
   "source": [
    "ww, bb, V, bk = fit_nn_gradopt(X_train, y_train, 30, best_k, params=\"rnd\")\n",
    "pred_test = nn_cost([ww, bb, V, bk], X_test)\n",
    "RMSE_test = np.sqrt(np.mean((y_test[:,None]-pred_test[:,None])**2))\n",
    "print(f'For K={best_k} the RMSE on the test set is: {RMSE_test}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 15.280000000000001 to be the regularisation parameter, Î±, that leads to the lowest validation RMSE of 0.7844591075708631.\n",
      "For Î±=15.280000000000001 the RMSE on the test set is: 0.8050006490522541.\n"
     ]
    }
   ],
   "source": [
    "# Using alvaroÂ´s relu backpropagation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
